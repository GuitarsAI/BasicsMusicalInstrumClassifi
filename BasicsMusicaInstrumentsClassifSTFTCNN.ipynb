{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Very Basics of Musical Instruments Classification using Machine Learning\n",
    "## Short-Time Fourier Transform (STFT) and Convolutional Neural Networks (CNN) \n",
    "\n",
    "<br>\n",
    "\n",
    "<p align=\"left\">\n",
    "<img src=\"./img/businesscard.jpg\" width=\"300px\" alt=\"Business Card\" align=\"left\" >\n",
    "</p>\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "#General\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# System\n",
    "import os, fnmatch\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import seaborn \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import HTML, display, Image\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib \n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import History, EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "# Random Seed\n",
    "from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "seed(0)\n",
    "set_random_seed(0)\n",
    "\n",
    "# Audio\n",
    "import librosa.display, librosa\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Configurations\n",
    "path='./audio/london_phill_dataset_multi/'\n",
    "\n",
    "# Display CPUs and GPUs\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations for Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    !git clone https://github.com/GuitarsAI/BasicsMusicalInstrumClassifi\n",
    "    !unzip ./BasicsMusicalInstrumClassifi/audio/*.zip -d ./BasicsMusicalInstrumClassifi/audio\n",
    "    path=\"./BasicsMusicalInstrumClassifi/audio/london_phill_dataset_multi/\"\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
    "    if \"GPU:0\" in tf.test.gpu_device_name():\n",
    "        config = tf.ConfigProto(log_device_placement=True, allow_soft_placement=True, device_count = {'GPU': 0})\n",
    "        config.gpu_options.allow_growth = True\n",
    "        session = tf.Session(config=config)\n",
    "        set_session(session)\n",
    "    else:\n",
    "        print(\"No GPU Detected. Configure the Runtime.\")\n",
    "except Exception as e:\n",
    "    print(\"Not inside Google Colab: %s. Using standard configurations.\" % (e))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal Processing Parameters\n",
    "fs = 44100         # Sampling Frequency\n",
    "n_fft = 2048       # length of the FFT window\n",
    "hop_length = 512   # Number of samples between successive frames\n",
    "\n",
    "\n",
    "# Machine Learning Parameters\n",
    "testset_size = 0.25 #Percentage of data for Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aux Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Display a Website\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "def show_web(url):\n",
    "    html_code='<center><iframe src=\"%s\" width=\"800\" height=\"600\" frameborder=\"0\" marginheight=\"0\" marginwidth=\"0\">Loading...</iframe></center>' \\\n",
    "\t\t% (url)\n",
    "    display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Audio Files, Generate Labels and Get Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Audio Files\n",
    "files = []\n",
    "labels =[]\n",
    "duration = []\n",
    "classes=['flute','sax','oboe', 'cello','trumpet','viola']\n",
    "for root, dirnames, filenames in os.walk(path):\n",
    "    for i, filename in enumerate(fnmatch.filter(filenames, '*.mp3')):\n",
    "        files.append(os.path.join(root, filename))\n",
    "        for name in classes:\n",
    "            if fnmatch.fnmatchcase(filename, '*'+name+'*'):\n",
    "                labels.append(name)\n",
    "                break\n",
    "        else:\n",
    "            labels.append('other')\n",
    "        print (\"Get %d = %s\"%(i+1, filename))\n",
    "        try:\n",
    "            y, sr = librosa.load(files[i], sr=fs)\n",
    "            if len(y) < 2:\n",
    "                print(\"Error loading %s\" % filename)\n",
    "                continue\n",
    "            #y/=y.max() #Normalize\n",
    "            yt, index = librosa.effects.trim(y,top_db=60) #Trim\n",
    "            duration.append(librosa.get_duration(yt, sr=fs))\n",
    "        except Exception as e:\n",
    "            print(\"Error loading %s. Error: %s\" % (filename,e))\n",
    "\n",
    "\n",
    "print(\"found %d audio files in %s\"%(len(files),path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max. Duration:\", max(duration))\n",
    "print(\"Min. Duration:\", min(duration))\n",
    "print(\"Average Duration:\", np.mean(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim Silence and Recalculate Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio files, trim silence and calculate duration\n",
    "duration = []\n",
    "for i,f in enumerate(files):\n",
    "    print (\"Get %d  %s\"%(i+1, f))\n",
    "    try:\n",
    "        y, sr = librosa.load(f, sr=fs)\n",
    "        if len(y) < 2:\n",
    "            print(\"Error loading %s\" % f)\n",
    "            continue\n",
    "        #y/=y.max() #Normalize\n",
    "        yt, index = librosa.effects.trim(y,top_db=60) #Trim\n",
    "        duration.append(librosa.get_duration(yt, sr=fs))\n",
    "    except Exception as e:\n",
    "        print(\"Error loading %s. Error: %s\" % (f,e))\n",
    "        \n",
    "print(\"Calculated %d Durations\"%len(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duration Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durationDist = pd.Series(np.array(duration))\n",
    "plt.figure()\n",
    "durationDist.plot.hist(grid=True, bins=40, rwidth=0.8,\n",
    "                   color='#607c8e')\n",
    "plt.title('Duration Distribution')\n",
    "plt.xlabel('Duration [s]')\n",
    "plt.ylabel('Counts')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "print(\"Duration average:\",np.mean(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short-Time Fourier Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_web(\"https://en.wikipedia.org/wiki/Short-time_Fourier_transform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STFT Example\n",
    "y, sr = librosa.load(files[10], sr=fs, duration=1)\n",
    "y/=y.max() #Normalize\n",
    "duration_in_samples=librosa.time_to_samples(1, sr=fs)\n",
    "y_pad = librosa.util.fix_length(y, duration_in_samples) #Pad to 1s if smaller\n",
    "y_stft=librosa.core.stft(y_pad, n_fft=n_fft, hop_length=hop_length)\n",
    "y_spec=librosa.amplitude_to_db(abs(y_stft), np.max)\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.title(\"Short-Time Fourier Transform Spectogram \\n %s\"%files[0])\n",
    "librosa.display.specshow(y_spec,sr=fs,y_axis='log', x_axis='time')\n",
    "plt.colorbar(format='%+2.0f dB');\n",
    "print(\"Spectogram Array Shape:\",y_spec.shape)\n",
    "ipd.Audio(y, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Labels\n",
    "labelencoder = LabelEncoder()\n",
    "labelencoder.fit(labels)\n",
    "print(len(labelencoder.classes_), \"classes:\", \", \".join(list(labelencoder.classes_)))\n",
    "classes_num = labelencoder.transform(labels)\n",
    "\n",
    "#OneHotEncoding\n",
    "encoder=OneHotEncoder(sparse=False, categories=\"auto\")\n",
    "onehot_labels=encoder.fit_transform(classes_num.reshape(len(classes_num),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train and Test Sets\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=testset_size, random_state=0)\n",
    "splits = splitter.split(files, onehot_labels)\n",
    "files_arr=np.array(files)\n",
    "\n",
    "for train_index, test_index in splits:\n",
    "    train_set_files = files_arr[train_index]\n",
    "    test_set_files = files_arr[test_index]\n",
    "    train_classes = onehot_labels[train_index]\n",
    "    test_classes = onehot_labels[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_web(\"https://en.wikipedia.org/wiki/Convolutional_neural_network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model\n",
    "model = Sequential()\n",
    "\n",
    "conv_filters =  16  # number of convolution filters\n",
    "\n",
    "# Layer 1\n",
    "model.add(Convolution2D(conv_filters, 3,input_shape=(1025, 87, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(Dropout(0.40)) \n",
    "\n",
    "# Layer 2\n",
    "model.add(Convolution2D(conv_filters, 3))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.40))\n",
    "\n",
    "# Flatten\n",
    "model.add(Flatten()) \n",
    "\n",
    "# Full layer\n",
    "model.add(Dense(16, activation='sigmoid')) \n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(6,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function \n",
    "loss = 'categorical_crossentropy' \n",
    "\n",
    "# Optimizer = Gradient Descent\n",
    "optimizer = 'sgd' \n",
    "\n",
    "# Compile\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureGenerator(files, labels):\n",
    "    while True:\n",
    "        for i,f in enumerate(files):\n",
    "            try:\n",
    "                feature_vectors = []\n",
    "                label = []\n",
    "                y, sr = librosa.load(f, sr=fs, duration=1)\n",
    "                if len(y) < 2:\n",
    "                    print(\"Error loading %s\" % f)\n",
    "                    continue\n",
    "                y/=y.max() #Normalize\n",
    "                duration_in_samples=librosa.time_to_samples(1, sr=fs)\n",
    "                y_pad = librosa.util.fix_length(y, duration_in_samples) #Pad to same duration\n",
    "                y_stft=librosa.core.stft(y_pad, n_fft=n_fft, hop_length=hop_length)\n",
    "                y_spec=librosa.amplitude_to_db(abs(y_stft), np.min)\n",
    "                scaler = StandardScaler()\n",
    "                dtype = K.floatx()\n",
    "                data = scaler.fit_transform(y_spec).astype(dtype)\n",
    "                data = np.expand_dims(data, axis=0)\n",
    "                data = np.expand_dims(data, axis=3)\n",
    "                feature_vectors.append(data)\n",
    "                label.append([labels[i]])\n",
    "                yield feature_vectors, label\n",
    "            except Exception as e:\n",
    "                print(\"Error loading %s. Error: %s\" % (f,e))\n",
    "                raise\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hist = History();\n",
    "es = EarlyStopping(monitor='val_acc', min_delta=0.01, restore_best_weights=True, patience= 10, verbose=1 )\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc',save_best_only=True, verbose=1)\n",
    "\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    tbc=TensorBoardColab()\n",
    "    callbacksKeras=[hist,es,mc,TensorBoardColabCallback(tbc)]\n",
    "\n",
    "except Exception as e:\n",
    "    callbacksKeras=[hist,es,mc]\n",
    "    print(\"Not inside Google Colab: %s. Using standard configurations.\" % (e))\n",
    "\n",
    "\n",
    "model.fit_generator(featureGenerator(train_set_files, train_classes), \n",
    "                    validation_data=(featureGenerator(test_set_files, test_classes)), \n",
    "                    validation_steps=150, \n",
    "                    steps_per_epoch=450,epochs=30,callbacks=callbacksKeras, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    ## As loss always exists\n",
    "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "    \n",
    "    ## Loss\n",
    "    plt.figure(1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training loss')\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation loss')\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    ## Accuracy\n",
    "    plt.figure(2)\n",
    "    for l in acc_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy')\n",
    "    for l in val_acc_list:    \n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy')\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = load_model('best_model.h5')\n",
    "test_pred = saved_model.predict_generator(featureGenerator(test_set_files, test_classes), steps=150,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_round=np.around(test_pred).astype('int');\n",
    "predictions_int=np.argmax(predictions_round,axis=1);\n",
    "predictions_labels=labelencoder.inverse_transform(np.ravel(predictions_int));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall - the ability of the classifier to find all the positive samples\n",
    "print(\"Recall: \", recall_score(classes_num[test_index], predictions_int,average=None))\n",
    "\n",
    "# Precision - The precision is intuitively the ability of the classifier not to \n",
    "#label as positive a sample that is negative\n",
    "print(\"Precision: \", precision_score(classes_num[test_index], predictions_int,average=None))\n",
    "\n",
    "# F1-Score - The F1 score can be interpreted as a weighted average of the precision \n",
    "#and recall\n",
    "print(\"F1-Score: \", f1_score(classes_num[test_index], predictions_int, average=None))\n",
    "\n",
    "# Accuracy - the number of correctly classified samples\n",
    "print(\"Accuracy: %.2f  ,\" % accuracy_score(classes_num[test_index], predictions_int,normalize=True), accuracy_score(classes_num[test_index], predictions_int,normalize=False) )\n",
    "print(\"Number of samples:\",classes_num[test_index].shape[0])\n",
    "\n",
    "print(classification_report(classes_num[test_index], predictions_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(classes_num[test_index], predictions_int)\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Plot Confusion Matrix\n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \"\"\"\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(16,12))\n",
    "plot_confusion_matrix(cnf_matrix, classes=labelencoder.classes_,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find wrong predicted samples indexes\n",
    "wrong_predictions = [i for i, (e1, e2) in enumerate(zip(classes_num[test_index], predictions_int)) if e1 != e2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find wrong predicted audio files\n",
    "print(np.array(labels)[test_index[wrong_predictions]])\n",
    "print(predictions_labels[wrong_predictions].T)\n",
    "print(np.array(files)[test_index[wrong_predictions]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
